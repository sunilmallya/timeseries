{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Timeseries SageMaker Template with Gluon\n",
    "\n",
    "This is a template to run the human activity recognition notebook. Refer the `smartphone_human_activity_classification_gluon.ipynb` for non sagemaker version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Package Data\n",
    "\n",
    "1. Load your train and test data as numpy arrays\n",
    "2. Package data as a pickle file and upload to S3\n",
    "\n",
    "by doing this, we can use the generic_ts.py file to run any timeseries classification task with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def get_labels_from_csv(path):\n",
    "    values = []\n",
    "    with open(path, 'rb') as csvfile:\n",
    "        rd = csv.reader(csvfile, delimiter=',')\n",
    "        for row in rd:\n",
    "            values.append(float(row[0]))\n",
    "    return np.array(values).astype('float32')\n",
    "\n",
    "\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "\n",
    "path = '../data'\n",
    "\n",
    "train = [path + \"/har_data/train/%strain.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "test = [path + \"/har_data/test/%stest.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "\n",
    "\n",
    "def load_data(files):\n",
    "    arr = []\n",
    "    for fname in files:\n",
    "        with open(fname, 'r') as f:\n",
    "            rows = [row.replace('  ', ' ').strip().split(' ') for row in f]\n",
    "            arr.append([np.array(ele, dtype=np.float32) for ele in rows])\n",
    "    return np.transpose(np.array(arr), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 128, 9), (2947, 128, 9))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = load_data(train)\n",
    "X_test = load_data(test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_path = path + \"/har_data/train/y_train.txt\"\n",
    "y_train = get_labels_from_csv(y_train_path)\n",
    "\n",
    "y_test_path = path + \"/har_data/test/y_test.txt\"\n",
    "y_test = get_labels_from_csv(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 5.0, 0.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "print(min(y_train), max(y_train), min(y_test), max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir pkl_data\n",
    "cd pkl_data\n",
    "mkdir train\n",
    "mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([X_train, y_train], open('pkl_data/train/train.pkl', 'wb'))\n",
    "pickle.dump([X_test, y_test], open('pkl_data/test/test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2947, 128, 9), (2947,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t, y_t = pickle.load(open('pkl_data/test/test.pkl', \"rb\"))\n",
    "#X_t, y_t = pickle.load(open('test.pkl', \"rb\"))\n",
    "X_t.shape, y_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- end of data transformation, loading and packaging ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-148886336128\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='pkl_data', key_prefix='data/har_pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute cell below to view the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\r\n",
      "TimeSeries Classificataion SageMaker Template \r\n",
      "'''\r\n",
      "\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd, nd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import pickle\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "import os\r\n",
      "def find_file(root_path, file_name):\r\n",
      "    for root, dirs, files in os.walk(root_path):\r\n",
      "        if file_name in files:\r\n",
      "            return os.path.join(root, file_name)\r\n",
      "\r\n",
      "def detach(hidden):\r\n",
      "    if isinstance(hidden, (tuple, list)):\r\n",
      "        hidden = [i.detach() for i in hidden]\r\n",
      "    else:\r\n",
      "        hidden = hidden.detach()\r\n",
      "    return hidden\r\n",
      "\r\n",
      "class BaseRNNClassifier(mx.gluon.Block):\r\n",
      "    '''\r\n",
      "    Exensible RNN class with LSTM that can operate with MXNet NDArray iter or DataLoader.\r\n",
      "    Includes fit() function to mimic the symbolic fit() function\r\n",
      "    '''\r\n",
      "    \r\n",
      "    @classmethod\r\n",
      "    def get_data(cls, batch, iter_type, ctx):\r\n",
      "        ''' get data and label from the iterator/dataloader '''\r\n",
      "        if iter_type == 'mxiter':\r\n",
      "            X = batch.data[0].as_in_context(ctx)\r\n",
      "            y = batch.label[0].as_in_context(ctx)\r\n",
      "        elif iter_type in [\"numpy\", \"dataloader\"]:\r\n",
      "            X = batch[0].as_in_context(ctx)\r\n",
      "            y = batch[1].as_in_context(ctx)\r\n",
      "        else:\r\n",
      "            raise ValueError(\"iter_type must be mxiter or numpy\")\r\n",
      "        return X, y\r\n",
      "    \r\n",
      "    @classmethod\r\n",
      "    def get_all_labels(cls, data_iterator, iter_type):\r\n",
      "        if iter_type == 'mxiter':\r\n",
      "            pass\r\n",
      "        elif iter_type in [\"numpy\", \"dataloader\"]:\r\n",
      "            return data_iterator._dataset._label\r\n",
      "    \r\n",
      "    def __init__(self, ctx):\r\n",
      "        super(BaseRNNClassifier, self).__init__()\r\n",
      "        self.ctx = ctx\r\n",
      "        self.batch_size = 128\r\n",
      "\r\n",
      "    def build_model(self, n_out, rnn_size=128, n_layer=1, model_dir = False):\r\n",
      "        self.rnn_size = rnn_size\r\n",
      "        self.n_layer = n_layer\r\n",
      "        self.n_out = n_out\r\n",
      "        self.model_dir = model_dir\r\n",
      "        with self.name_scope():\r\n",
      "            self.lstm = mx.gluon.rnn.LSTM(self.rnn_size, self.n_layer, layout='NTC')\r\n",
      "            self.output = mx.gluon.nn.Dense(self.n_out)\r\n",
      "\r\n",
      "    def forward(self, x, hidden = False):\r\n",
      "        if not hidden:\r\n",
      "            init_state = mx.nd.zeros((self.n_layer, len(x), self.rnn_size), self.ctx)\r\n",
      "            hidden = [init_state] * 2\r\n",
      "        out, hidden = self.lstm(x, hidden)\r\n",
      "        out = out[:, out.shape[1]-1, :]\r\n",
      "        out = self.output(out)\r\n",
      "        return out, hidden\r\n",
      "    \r\n",
      "    def compile_model(self, loss=None, lr=3E-3):\r\n",
      "        self.collect_params().initialize(mx.init.Xavier(), ctx=self.ctx)\r\n",
      "        self.criterion = mx.gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "        self.loss = mx.gluon.loss.SoftmaxCrossEntropyLoss() if loss is None else loss\r\n",
      "        self.lr = lr\r\n",
      "        self.optimizer = mx.gluon.Trainer(self.collect_params(), 'adam', \r\n",
      "                                          {'learning_rate': self.lr})\r\n",
      "\r\n",
      "    def top_k_acc(self, data_iterator, iter_type='mxiter', top_k=3, batch_size=128):\r\n",
      "        self.batch_size = batch_size\r\n",
      "        batch_pred_list = []\r\n",
      "        true_labels = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            batch_pred = self.forward(data, hidden)\r\n",
      "            #batch_pred = mx.nd.argmax(batch_pred, axis=1)\r\n",
      "            batch_pred_list.append(batch_pred.asnumpy())\r\n",
      "            true_labels.append(label)\r\n",
      "        y = np.vstack(batch_pred_list)\r\n",
      "        true_labels = np.vstack(true_labels)\r\n",
      "        argsorted_y = np.argsort(y)[:,-top_k:]\r\n",
      "        return np.asarray(np.any(argsorted_y.T == true_labels, axis=0).mean(dtype='f'))\r\n",
      "    \r\n",
      "    def evaluate_accuracy(self, data_iterator, metric='acc', iter_type='mxiter', batch_size=128):\r\n",
      "        met = mx.metric.Accuracy()\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            # Lets do a forward pass only!\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            preds = mx.nd.argmax(output, axis=1)\r\n",
      "            met.update(labels=label, preds=preds)                \r\n",
      "        return met.get()                   \r\n",
      "                    \r\n",
      "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\r\n",
      "        batch_pred_list = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            batch_pred_list.append(output.asnumpy())\r\n",
      "        return np.argmax(np.vstack(batch_pred_list), 1)\r\n",
      "    \r\n",
      "    def fit(self, train_data, test_data, epochs, batch_size, verbose=True):\r\n",
      "        '''\r\n",
      "        @train_data:  can be of type list of Numpy array, DataLoader, MXNet NDArray Iter\r\n",
      "        '''\r\n",
      "        \r\n",
      "        self.batch_size = batch_size\r\n",
      "        moving_loss = 0.\r\n",
      "        total_batches = 0\r\n",
      "\r\n",
      "        train_loss = []\r\n",
      "        train_acc = []\r\n",
      "        test_acc = []\r\n",
      "\r\n",
      "        iter_type = 'numpy'\r\n",
      "        train_iter = None\r\n",
      "        test_iter = None\r\n",
      "        #print \"Data type:\", type(train_data), type(test_data), iter_type, type(train_data[0])\r\n",
      "        \r\n",
      "        # Can take MX NDArrayIter, or DataLoader\r\n",
      "        if isinstance(train_data, mx.io.NDArrayIter):\r\n",
      "            train_iter = train_data\r\n",
      "            test_iter = test_data\r\n",
      "            iter_type = 'mxiter'\r\n",
      "            #total_batches = train_iter.num_data // train_iter.batch_size\r\n",
      "\r\n",
      "        elif isinstance(train_data, list):\r\n",
      "            if isinstance(train_data[0], np.ndarray) and isinstance(train_data[1], np.ndarray):\r\n",
      "                X, y = np.asarray(train_data[0]).astype('float32'), np.asarray(train_data[1]).astype('float32')\r\n",
      "                tX, ty = np.asarray(test_data[0]).astype('float32'), np.asarray(test_data[1]).astype('float32')\r\n",
      "                \r\n",
      "                total_batches = X.shape[0] // batch_size\r\n",
      "                train_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y), \r\n",
      "                                    batch_size=batch_size, shuffle=True, last_batch='discard')\r\n",
      "                test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(tX, ty), \r\n",
      "                                    batch_size=batch_size, shuffle=False, last_batch='discard')\r\n",
      "                \r\n",
      "        elif isinstance(train_data, mx.gluon.data.dataloader.DataLoader) and isinstance(test_data, mx.gluon.data.dataloader.DataLoader):\r\n",
      "            train_iter = train_data\r\n",
      "            test_iter = test_data\r\n",
      "            total_batches = len(train_iter)\r\n",
      "        else:\r\n",
      "            raise ValueError(\"pass mxnet ndarray or numpy array as [data, label]\")\r\n",
      "        \r\n",
      "        best_acc = 0.0\r\n",
      "        for e in range(epochs):\r\n",
      "\r\n",
      "            # reset iterators if of MXNet Itertype\r\n",
      "            if iter_type == \"mxiter\":\r\n",
      "                train_iter.reset()\r\n",
      "                test_iter.reset()\r\n",
      "        \r\n",
      "            init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "            hidden = [init_state] * 2                \r\n",
      "            yhat = []\r\n",
      "            for i, batch in enumerate(train_iter):\r\n",
      "                data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "                hidden = detach(hidden)\r\n",
      "                with autograd.record():\r\n",
      "                    preds, hidden = self.forward(data, hidden)\r\n",
      "                    loss = self.loss(preds, label) \r\n",
      "                    yhat.extend(preds)\r\n",
      "                loss.backward()                                        \r\n",
      "                self.optimizer.step(batch_size)\r\n",
      "                preds = mx.nd.argmax(preds, axis=1)\r\n",
      "                \r\n",
      "                batch_acc = mx.nd.mean(preds == label).asscalar()\r\n",
      "\r\n",
      "                if i == 0:\r\n",
      "                    moving_loss = nd.mean(loss).asscalar()\r\n",
      "                else:\r\n",
      "                    moving_loss = .99 * moving_loss + .01 * mx.nd.mean(loss).asscalar()\r\n",
      "                    \r\n",
      "                if verbose and i%100 == 0:\r\n",
      "                    print('[Epoch {}] [Batch {}/{}] Loss: {:.5f}, Batch acc: {:.5f}'.format(\r\n",
      "                          e, i, total_batches, moving_loss, batch_acc))                    \r\n",
      "                    \r\n",
      "            train_loss.append(moving_loss)\r\n",
      "            \r\n",
      "            t_acc = self.evaluate_accuracy(train_iter, iter_type=iter_type, batch_size=batch_size)\r\n",
      "            train_acc.append(t_acc[1])\r\n",
      "            \r\n",
      "            tst_acc = self.evaluate_accuracy(test_iter, iter_type=iter_type, batch_size=batch_size)\r\n",
      "            test_acc.append(tst_acc[1])\r\n",
      "\r\n",
      "            print(\"Epoch %s. Loss: %.5f Train Acc: %s Test Acc: %s\" % (e, moving_loss, t_acc, tst_acc))\r\n",
      "            if best_acc < tst_acc and self.model_dir:\r\n",
      "                print(\"=-=-=-=-=-=-=Model Saved=-=-=-=-=-=-=\")\r\n",
      "                best_acc = tst_acc\r\n",
      "                self.save_params('{}/model_best.params'.format(self.model_dir))\r\n",
      "        return train_loss, train_acc, test_acc\r\n",
      "                    \r\n",
      "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\r\n",
      "        batch_pred_list = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            batch_pred_list.append(output.asnumpy())\r\n",
      "        return np.argmax(np.vstack(batch_pred_list), 1)\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def train(channel_input_dirs, model_dir, hyperparameters, **kwargs):\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.1)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 100)\r\n",
      "    num_gpus = hyperparameters.get('num_gpus', 0)\r\n",
      "    \r\n",
      "    # Parametrize the network definition\r\n",
      "    n_out = hyperparameters.get('n_out', 2)\r\n",
      "    rnn_size = hyperparameters.get('rnn_size', 64)\r\n",
      "    n_layer = hyperparameters.get('n_layer', 1)\r\n",
      "        \r\n",
      "    #path = os.path.join(channel_input_dirs)\r\n",
      "    path = channel_input_dirs['training']\r\n",
      "    X_train, y_train = load_data(path, 'train')\r\n",
      "    X_test, y_test = load_data(path, 'test')\r\n",
      "    \r\n",
      "    # context \r\n",
      "    ctx = mx.cpu()\r\n",
      "    if num_gpus >= 1:\r\n",
      "        ctx = mx.gpu()\r\n",
      "    \r\n",
      "    model = BaseRNNClassifier(ctx)\r\n",
      "    model.build_model(n_out=n_out, rnn_size=rnn_size, n_layer=n_layer, model_dir = model_dir)\r\n",
      "    model.compile_model()\r\n",
      "    train_loss, train_acc, test_acc = model.fit([X_train, y_train], [X_test, y_test], batch_size=batch_size, epochs=epochs)\r\n",
      "    return model\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    net.save_params('{}/model.params'.format(model_dir))\r\n",
      "    '''\r\n",
      "    These parameters need to be saved.\r\n",
      "    '''\r\n",
      "    f = open('{}/model.json'.format(model_dir), 'w')\r\n",
      "    json.dump({'rnn_size': net.rnn_size,\r\n",
      "               'n_layer': net.n_layer,\r\n",
      "               'n_out': net.n_out},\r\n",
      "              f)\r\n",
      "    f.close()\r\n",
      "\r\n",
      "## Load Train and Test Data\r\n",
      "def load_data(path, typ):\r\n",
      "    if typ == \"train\":\r\n",
      "        #Load Train Data\r\n",
      "        f = find_file(path + '/train', \"train.pkl\")\r\n",
      "    else:#Load Test Data\r\n",
      "        f = find_file(path + '/test', \"test.pkl\")\r\n",
      "    X_t, y_t = pickle.load(open(f, \"rb\"))\r\n",
      "    return X_t, y_t\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    f = open('{}/model.json'.format(model_dir), 'r')\r\n",
      "    block_params = json.load(f)\r\n",
      "    f.close()\r\n",
      "    model = BaseRNNClassifier(ctx)\r\n",
      "    model.build_model(n_out=block_params['n_out'], \r\n",
      "                      rnn_size=block_params['rnn_size'], \r\n",
      "                      n_layer=block_params['n_layer'])\r\n",
      "    model.compile_model()\r\n",
      "    model.load_params('{}/model_best.params'.format(model_dir), ctx)\r\n",
      "    \r\n",
      "    return model\r\n",
      "    \r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.#\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    # data: <type 'unicode'>\r\n",
      "    \r\n",
      "    parsed = json.loads(data) #<type 'list'>\r\n",
      "\r\n",
      "    nda = mx.nd.array(np.array(parsed), ctx = mx.cpu())\r\n",
      "\r\n",
      "    output, _ = net(nda) # calling model.forward()\r\n",
      "    prediction = mx.nd.argmax(output, axis=1)\r\n",
      "    \r\n",
      "    response_body = json.dumps(prediction.asnumpy().tolist())\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat generic_ts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "\n",
    "The ```MXNet``` class allows us to run our training function on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on a single m4.xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet(\"generic_ts.py\", \n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.m4.xlarge\",\n",
    "          hyperparameters={'batch_size': 32, \n",
    "                           'epochs': 4, \n",
    "                           'n_out': len(LABELS),\n",
    "                           'rnn_size': 64,\n",
    "                           'n_layer': 1,\n",
    "                           'num_gpus': 0\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-148886336128\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-05-24-23-36-48-789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................\n",
      "\u001b[31m2018-05-24 23:39:31,613 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:31,613 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:31,620 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:33,811 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'current_host': u'algo-1', u'hosts': [u'algo-1']}, 'user_script_name': u'generic_ts.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'generic_ts.py', u'rnn_size': 64, u'num_gpus': 0, u'n_layer': 1, u'batch_size': 32, u'epochs': 4, u'sagemaker_submit_directory': u's3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-2018-05-24-23-36-48-789/source/sourcedir.tar.gz', u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-2018-05-24-23-36-48-789', u'sagemaker_container_log_level': 20, u'n_out': 6}, 'hosts': [u'algo-1'], 'job_name': 'sagemaker-mxnet-2018-05-24-23-36-48-789', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-2018-05-24-23-36-48-789/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-east-1', '_scheduler_ip': '10.32.0.4', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-2018-05-24-23-36-48-789/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:33,926 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:34,036 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-east-1-148886336128.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-24 23:39:34,155 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 0/229] Loss: 1.82064, Batch acc: 0.06250\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 100/229] Loss: 1.46271, Batch acc: 0.46875\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 200/229] Loss: 1.03790, Batch acc: 0.71875\u001b[0m\n",
      "\u001b[31mEpoch 0. Loss: 1.00054 Train Acc: ('accuracy', 0.632778384279476) Test Acc: ('accuracy', 0.5930706521739131)\u001b[0m\n",
      "\u001b[31m=-=-=-=-=-=-=Model Saved=-=-=-=-=-=-=\u001b[0m\n",
      "\u001b[31m[Epoch 1] [Batch 0/229] Loss: 0.93319, Batch acc: 0.62500\u001b[0m\n",
      "\u001b[31m[Epoch 1] [Batch 100/229] Loss: 0.77926, Batch acc: 0.75000\u001b[0m\n",
      "\u001b[31m[Epoch 1] [Batch 200/229] Loss: 0.58731, Batch acc: 0.87500\u001b[0m\n",
      "\u001b[31mEpoch 1. Loss: 0.54469 Train Acc: ('accuracy', 0.8694050218340611) Test Acc: ('accuracy', 0.8070652173913043)\u001b[0m\n",
      "\u001b[31m=-=-=-=-=-=-=Model Saved=-=-=-=-=-=-=\u001b[0m\n",
      "\u001b[31m[Epoch 2] [Batch 0/229] Loss: 0.28975, Batch acc: 0.90625\u001b[0m\n",
      "\u001b[31m[Epoch 2] [Batch 100/229] Loss: 0.29260, Batch acc: 0.90625\u001b[0m\n",
      "\u001b[31m[Epoch 2] [Batch 200/229] Loss: 0.26882, Batch acc: 0.90625\u001b[0m\n",
      "\u001b[31mEpoch 2. Loss: 0.33198 Train Acc: ('accuracy', 0.7920305676855895) Test Acc: ('accuracy', 0.7693614130434783)\u001b[0m\n",
      "\u001b[31m[Epoch 3] [Batch 0/229] Loss: 0.31921, Batch acc: 0.90625\u001b[0m\n",
      "\u001b[31m[Epoch 3] [Batch 100/229] Loss: 0.37755, Batch acc: 0.84375\u001b[0m\n",
      "\u001b[31m[Epoch 3] [Batch 200/229] Loss: 0.33221, Batch acc: 0.90625\u001b[0m\n",
      "\u001b[31mEpoch 3. Loss: 0.31180 Train Acc: ('accuracy', 0.9417303493449781) Test Acc: ('accuracy', 0.8800951086956522)\u001b[0m\n",
      "\u001b[31m=-=-=-=-=-=-=Model Saved=-=-=-=-=-=-=\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 677\n"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2018-05-24-23-36-48-789\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-mxnet-2018-05-24-23-36-48-789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction with data to verify the endpoint is up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results:\n",
      "[4.0, 0.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 3.0, 2.0, 0.0, 4.0, 2.0, 0.0, 3.0, 1.0, 5.0, 4.0, 0.0, 3.0]\n",
      "Ground truth results:\n",
      "[4.0, 0.0, 3.0, 1.0, 0.0, 3.0, 1.0, 0.0, 3.0, 2.0, 0.0, 3.0, 1.0, 0.0, 3.0, 1.0, 5.0, 4.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "preds = predictor.predict(X_t[0:2000:100])\n",
    "print('Prediction results:')\n",
    "print(preds)\n",
    "print('Ground truth results:')\n",
    "print(y_t[0:2000:100].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the **SageMaker Endpoint:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-mxnet-2018-05-24-23-36-48-789\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
